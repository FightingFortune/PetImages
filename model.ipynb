{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Rearranging data...\n",
      "completed: 25004 items added to training data\n",
      "x_train: 20004  |  (20004, 75, 75, 1)\n",
      "y_train: 20004\n",
      "x_val: 5000\n",
      "y_val: 5000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "LABELS = ['CAT', 'DOG']\n",
    "\n",
    "X = []\n",
    "print(\"Reading data...\")\n",
    "for directory in LABELS:\n",
    "    path = os.path.join(directory, \"resized\")\n",
    "    for img_name in os.listdir(path):\n",
    "        img = cv2.imread(os.path.join(path, img_name), cv2.IMREAD_GRAYSCALE)\n",
    "        X.append([np.array(img), np.eye(2)[LABELS.index(directory)]])\n",
    "\n",
    "print(\"Rearranging data...\")\n",
    "np.random.shuffle(X)\n",
    "x_train = []\n",
    "y_train = []\n",
    "for img in X:\n",
    "    x_train.append(img[0])\n",
    "    y_train.append(img[1])\n",
    "if len(x_train) == len(y_train):\n",
    "    slice_index = int(len(x_train)*0.2)\n",
    "    x_val = x_train[:slice_index]\n",
    "    y_val = y_train[:slice_index]\n",
    "    x_train = x_train[slice_index:]\n",
    "    y_train = y_train[slice_index:]\n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    x_val = np.array(x_val)\n",
    "    x_train = x_train.reshape(x_train.shape[0], 75, 75, 1)\n",
    "    x_val = x_val.reshape(x_val.shape[0], 75, 75, 1)\n",
    "    \n",
    "    #normalize image data\n",
    "    x_train = np.true_divide(x_train, 255)\n",
    "    x_val = np.true_divide(x_val, 255)\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_val = np.array(y_val)\n",
    "        \n",
    "    \n",
    "    print(f\"completed: {len(X)} items added to training data\")\n",
    "    print(f\"x_train: {len(x_train)}  |  {x_train.shape}\")\n",
    "    print(f\"y_train: {len(y_train)}\")\n",
    "    print(f\"x_val: {len(x_val)}\")\n",
    "    print(f\"y_val: {len(y_val)}\")\n",
    "else:\n",
    "    print(\"something went wrong with the data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "float64\n",
      "['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "print(y_val[0])\n",
    "print(x_train[0].dtype)\n",
    "print(dir(x_train[0]))\n",
    "#img = Image.fromarray(x_train[1], mode='L')\n",
    "#cv2.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1102 19:57:57.045602 20568 training_utils.py:1210] When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 73, 73, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2367616   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,386,690\n",
      "Trainable params: 2,386,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1102 19:57:57.644145 20568 deprecation.py:323] From C:\\Users\\Jaycop\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1393: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1102 19:57:57.733226 20568 deprecation.py:323] From C:\\Users\\Jaycop\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py:466: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Apply a constraint manually following the optimizer update step.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20004/20004 [==============================] - 348s 17ms/sample - loss: nan - accuracy: 0.5024 - categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.4904 - val_categorical_crossentropy: nan\n",
      "Epoch 2/4\n",
      "20004/20004 [==============================] - 344s 17ms/sample - loss: nan - accuracy: 0.5024 - categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.4904 - val_categorical_crossentropy: nan\n",
      "Epoch 3/4\n",
      "20004/20004 [==============================] - 347s 17ms/sample - loss: nan - accuracy: 0.5024 - categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.4904 - val_categorical_crossentropy: nan\n",
      "Epoch 4/4\n",
      "20004/20004 [==============================] - 354s 18ms/sample - loss: nan - accuracy: 0.5024 - categorical_crossentropy: nan - val_loss: nan - val_accuracy: 0.4904 - val_categorical_crossentropy: nan\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), activation='relu', input_shape=(75,75,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(LABELS), activation='relu'))\n",
    "\n",
    "#train\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy', 'categorical_crossentropy'])\n",
    "print('compiled.')\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train, batch_size=25, epochs=EPOCHS, verbose=1, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_MutableMapping__marker', '__abstractmethods__', '__bool__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__setitem__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_d', '_e', '_gcpl_crt_order', '_id', '_lapl', '_lcpl', '_swmr_mode', 'attrs', 'clear', 'close', 'copy', 'create_dataset', 'create_dataset_like', 'create_group', 'create_virtual_dataset', 'driver', 'fid', 'file', 'filename', 'flush', 'get', 'id', 'items', 'keys', 'libver', 'mode', 'move', 'name', 'parent', 'pop', 'popitem', 'ref', 'regionref', 'require_dataset', 'require_group', 'setdefault', 'swmr_mode', 'update', 'userblock_size', 'values', 'visit', 'visititems']\n",
      "{'loss': [nan, nan, nan, nan, nan, nan, nan, nan], 'accuracy': [0.49815038, 0.4980504, 0.4980504, 0.4980504, 0.4980504, 0.4980504, 0.4980504, 0.4980504], 'categorical_crossentropy': [nan, nan, nan, nan, nan, nan, nan, nan], 'val_loss': [nan, nan, nan, nan, nan, nan, nan, nan], 'val_accuracy': [0.5078, 0.5078, 0.5078, 0.5078, 0.5078, 0.5078, 0.5078, 0.5078], 'val_categorical_crossentropy': [nan, nan, nan, nan, nan, nan, nan, nan]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "pyplot.plot(history.history[\"accuracy\"])\n",
    "\n",
    "pyplot.show()\n",
    "print(history.history['accuracy'])\n",
    "\"\"\"\n",
    "import h5py\n",
    "import time\n",
    "h5 =  h5py.File(f\"catvsdog-{int(time.time())}.hdf5\", \"w\")\n",
    "print(dir(h5))\n",
    "print(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
